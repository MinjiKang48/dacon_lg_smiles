{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 필요 패키지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit import RDLogger\n",
    "RDLogger.DisableLog('rdApp.*')  \n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from util import *\n",
    "from models import Encoder, DecoderWithAttention\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in pytorch, for image\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "# cuda\n",
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SmilesDataset import *\n",
    "import torch.backends.cudnn as cudnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 120  # number of epochs to train for (if early stopping is not triggered)\n",
    "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "batch_size = 128\n",
    "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "print_freq = 100  # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "checkpoint = None  # path to checkpoint, None if none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 17/1907741 [00:00<00:32, 59518.50it/s]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2e84efdfc706>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                      std=[0.229, 0.224, 0.225])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSmilesDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'TRAIN'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCompose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/notebook/Experiments/Seona/dacon/SmilesDataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data_folder, data_name, split, transform)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstoi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mchar\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m80\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_folder = './'#'/media/ksk/Backup/SMILES dataset/'\n",
    "data_name = 'extra_train.csv'\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "dataset = SmilesDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(dataset.get_vocab()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, val_set = torch.utils.data.random_split(dataset, [int(len(dataset) * 0.98), len(dataset) - int(len(dataset) * 0.98)])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
    "                                       embed_dim=emb_dim,\n",
    "                                       decoder_dim=decoder_dim,\n",
    "                                       vocab_size=len(dataset.get_vocab()[0]),\n",
    "                                       dropout=dropout)\n",
    "decoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
    "                                     lr=decoder_lr)\n",
    "encoder = Encoder()\n",
    "encoder.fine_tune(fine_tune_encoder)\n",
    "encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                     lr=encoder_lr) if fine_tune_encoder else None\n",
    "\n",
    "\n",
    "# Move to GPU, if available\n",
    "decoder = decoder.to(device)\n",
    "encoder = encoder.to(device)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Performs one epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss (per word decoded)\n",
    "    top5accs = AverageMeter()  # top5 accuracy\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, (imgs, caps, caplens) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0] # 5045, 69\n",
    "        targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0] # 5045\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time, loss=losses,\n",
    "                                                                          top5=top5accs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :return: BLEU-4 score\n",
    "    \"\"\"\n",
    "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "    hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
    "    # solves the issue #57\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, (imgs, caps, caplens) in enumerate(val_loader):\n",
    "\n",
    "            # Move to device, if available\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            caplens = caplens.to(device)\n",
    "\n",
    "            # Forward prop.\n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            # Remove timesteps that we didn't decode at, or are pads\n",
    "            # pack_padded_sequence is an easy trick to do this\n",
    "            scores_copy = scores.clone()\n",
    "            scores = pack_padded_sequence(scores, decode_lengths, batch_first=True)[0]\n",
    "            targets = pack_padded_sequence(targets, decode_lengths, batch_first=True)[0]\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            # Keep track of metrics\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5 = accuracy(scores, targets, 5)\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "            \n",
    "            # bleu4\n",
    "            #bleu4 = (references, hypothesis) \n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
    "                                                                                loss=losses, top5=top5accs))\n",
    "            \n",
    "\n",
    "\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}\\n'.format( # , BLEU-4 - {bleu}\n",
    "                loss=losses,\n",
    "                top5=top5accs)) # ,bleu=bleu4\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = np.infty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/6958]\tBatch Time 1.205 (1.205)\tData Load Time 0.290 (0.290)\tLoss 4.9276 (4.9276)\tTop-5 Accuracy 6.332 (6.332)\n",
      "Epoch: [0][100/6958]\tBatch Time 0.821 (0.820)\tData Load Time 0.000 (0.003)\tLoss 2.1502 (2.8004)\tTop-5 Accuracy 90.300 (77.090)\n",
      "Epoch: [0][200/6958]\tBatch Time 0.846 (0.826)\tData Load Time 0.001 (0.002)\tLoss 1.8461 (2.4062)\tTop-5 Accuracy 93.695 (84.442)\n",
      "Epoch: [0][300/6958]\tBatch Time 0.830 (0.830)\tData Load Time 0.000 (0.001)\tLoss 1.7608 (2.2064)\tTop-5 Accuracy 94.677 (87.655)\n",
      "Epoch: [0][400/6958]\tBatch Time 0.862 (0.833)\tData Load Time 0.000 (0.001)\tLoss 1.6233 (2.0774)\tTop-5 Accuracy 95.608 (89.536)\n",
      "Epoch: [0][500/6958]\tBatch Time 0.829 (0.834)\tData Load Time 0.000 (0.001)\tLoss 1.5936 (1.9833)\tTop-5 Accuracy 95.528 (90.807)\n",
      "Epoch: [0][600/6958]\tBatch Time 0.810 (0.848)\tData Load Time 0.001 (0.001)\tLoss 1.5231 (1.9107)\tTop-5 Accuracy 96.989 (91.746)\n",
      "Epoch: [0][700/6958]\tBatch Time 0.824 (0.846)\tData Load Time 0.001 (0.001)\tLoss 1.4997 (1.8516)\tTop-5 Accuracy 96.716 (92.474)\n",
      "Epoch: [0][800/6958]\tBatch Time 0.854 (0.846)\tData Load Time 0.001 (0.001)\tLoss 1.4143 (1.8017)\tTop-5 Accuracy 97.621 (93.064)\n",
      "Epoch: [0][900/6958]\tBatch Time 0.835 (0.845)\tData Load Time 0.000 (0.001)\tLoss 1.3783 (1.7599)\tTop-5 Accuracy 97.927 (93.541)\n",
      "Epoch: [0][1000/6958]\tBatch Time 0.838 (0.845)\tData Load Time 0.000 (0.001)\tLoss 1.3586 (1.7226)\tTop-5 Accuracy 97.891 (93.951)\n",
      "Epoch: [0][1100/6958]\tBatch Time 0.831 (0.845)\tData Load Time 0.000 (0.001)\tLoss 1.3711 (1.6903)\tTop-5 Accuracy 97.745 (94.300)\n",
      "Epoch: [0][1200/6958]\tBatch Time 0.863 (0.845)\tData Load Time 0.000 (0.001)\tLoss 1.3059 (1.6617)\tTop-5 Accuracy 97.984 (94.601)\n",
      "Epoch: [0][1300/6958]\tBatch Time 0.851 (0.844)\tData Load Time 0.000 (0.001)\tLoss 1.2879 (1.6356)\tTop-5 Accuracy 98.278 (94.869)\n",
      "Epoch: [0][1400/6958]\tBatch Time 0.851 (0.844)\tData Load Time 0.000 (0.001)\tLoss 1.2548 (1.6121)\tTop-5 Accuracy 98.482 (95.104)\n",
      "Epoch: [0][1500/6958]\tBatch Time 0.851 (0.844)\tData Load Time 0.000 (0.001)\tLoss 1.2867 (1.5905)\tTop-5 Accuracy 98.301 (95.314)\n",
      "Epoch: [0][1600/6958]\tBatch Time 0.862 (0.844)\tData Load Time 0.000 (0.001)\tLoss 1.2914 (1.5712)\tTop-5 Accuracy 98.368 (95.502)\n",
      "Epoch: [0][1700/6958]\tBatch Time 0.859 (0.844)\tData Load Time 0.000 (0.001)\tLoss 1.2615 (1.5531)\tTop-5 Accuracy 98.547 (95.673)\n",
      "Epoch: [0][1800/6958]\tBatch Time 0.854 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2780 (1.5365)\tTop-5 Accuracy 98.233 (95.829)\n",
      "Epoch: [0][1900/6958]\tBatch Time 0.841 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2371 (1.5208)\tTop-5 Accuracy 98.674 (95.970)\n",
      "Epoch: [0][2000/6958]\tBatch Time 0.845 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.1749 (1.5063)\tTop-5 Accuracy 98.760 (96.102)\n",
      "Epoch: [0][2100/6958]\tBatch Time 0.833 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2210 (1.4926)\tTop-5 Accuracy 98.584 (96.222)\n",
      "Epoch: [0][2200/6958]\tBatch Time 0.866 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2219 (1.4798)\tTop-5 Accuracy 98.551 (96.334)\n",
      "Epoch: [0][2300/6958]\tBatch Time 0.836 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2188 (1.4676)\tTop-5 Accuracy 98.648 (96.437)\n",
      "Epoch: [0][2400/6958]\tBatch Time 0.829 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.2093 (1.4562)\tTop-5 Accuracy 98.483 (96.534)\n",
      "Epoch: [0][2500/6958]\tBatch Time 0.871 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.1872 (1.4454)\tTop-5 Accuracy 98.633 (96.625)\n",
      "Epoch: [0][2600/6958]\tBatch Time 0.850 (0.844)\tData Load Time 0.000 (0.000)\tLoss 1.1842 (1.4351)\tTop-5 Accuracy 98.744 (96.710)\n",
      "Epoch: [0][2700/6958]\tBatch Time 0.834 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1840 (1.4255)\tTop-5 Accuracy 98.651 (96.788)\n",
      "Epoch: [0][2800/6958]\tBatch Time 0.858 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1460 (1.4163)\tTop-5 Accuracy 98.920 (96.863)\n",
      "Epoch: [0][2900/6958]\tBatch Time 0.854 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1605 (1.4073)\tTop-5 Accuracy 98.891 (96.934)\n",
      "Epoch: [0][3000/6958]\tBatch Time 0.855 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1159 (1.3988)\tTop-5 Accuracy 99.029 (97.001)\n",
      "Epoch: [0][3100/6958]\tBatch Time 0.821 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1721 (1.3908)\tTop-5 Accuracy 99.049 (97.064)\n",
      "Epoch: [0][3200/6958]\tBatch Time 0.821 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1700 (1.3831)\tTop-5 Accuracy 98.854 (97.123)\n",
      "Epoch: [0][3300/6958]\tBatch Time 0.835 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1553 (1.3757)\tTop-5 Accuracy 99.251 (97.180)\n",
      "Epoch: [0][3400/6958]\tBatch Time 0.852 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1257 (1.3684)\tTop-5 Accuracy 98.944 (97.235)\n",
      "Epoch: [0][3500/6958]\tBatch Time 0.827 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1239 (1.3615)\tTop-5 Accuracy 99.178 (97.287)\n",
      "Epoch: [0][3600/6958]\tBatch Time 0.836 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1358 (1.3549)\tTop-5 Accuracy 98.769 (97.335)\n",
      "Epoch: [0][3700/6958]\tBatch Time 0.827 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1419 (1.3485)\tTop-5 Accuracy 98.828 (97.382)\n",
      "Epoch: [0][3800/6958]\tBatch Time 0.836 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1069 (1.3423)\tTop-5 Accuracy 99.251 (97.428)\n",
      "Epoch: [0][3900/6958]\tBatch Time 0.859 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1204 (1.3362)\tTop-5 Accuracy 98.981 (97.471)\n",
      "Epoch: [0][4000/6958]\tBatch Time 0.853 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1181 (1.3304)\tTop-5 Accuracy 99.207 (97.512)\n",
      "Epoch: [0][4100/6958]\tBatch Time 0.830 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.1172 (1.3248)\tTop-5 Accuracy 99.155 (97.552)\n",
      "Epoch: [0][4200/6958]\tBatch Time 0.839 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0848 (1.3194)\tTop-5 Accuracy 99.417 (97.590)\n",
      "Epoch: [0][4300/6958]\tBatch Time 0.864 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0874 (1.3140)\tTop-5 Accuracy 99.094 (97.627)\n",
      "Epoch: [0][4400/6958]\tBatch Time 0.830 (0.843)\tData Load Time 0.001 (0.000)\tLoss 1.0950 (1.3089)\tTop-5 Accuracy 99.219 (97.662)\n",
      "Epoch: [0][4500/6958]\tBatch Time 0.864 (0.843)\tData Load Time 0.001 (0.000)\tLoss 1.0683 (1.3038)\tTop-5 Accuracy 99.267 (97.697)\n",
      "Epoch: [0][4600/6958]\tBatch Time 0.845 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0730 (1.2989)\tTop-5 Accuracy 99.201 (97.730)\n",
      "Epoch: [0][4700/6958]\tBatch Time 0.870 (0.843)\tData Load Time 0.001 (0.000)\tLoss 1.1197 (1.2943)\tTop-5 Accuracy 98.983 (97.762)\n",
      "Epoch: [0][4800/6958]\tBatch Time 0.818 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0473 (1.2897)\tTop-5 Accuracy 99.547 (97.792)\n",
      "Epoch: [0][4900/6958]\tBatch Time 0.848 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0493 (1.2852)\tTop-5 Accuracy 99.450 (97.822)\n",
      "Epoch: [0][5000/6958]\tBatch Time 0.856 (0.843)\tData Load Time 0.000 (0.000)\tLoss 1.0500 (1.2809)\tTop-5 Accuracy 99.220 (97.851)\n",
      "Epoch: [0][5100/6958]\tBatch Time 0.845 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0462 (1.2767)\tTop-5 Accuracy 99.297 (97.878)\n",
      "Epoch: [0][5200/6958]\tBatch Time 0.830 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0472 (1.2726)\tTop-5 Accuracy 99.339 (97.905)\n",
      "Epoch: [0][5300/6958]\tBatch Time 0.841 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0598 (1.2686)\tTop-5 Accuracy 99.193 (97.931)\n",
      "Epoch: [0][5400/6958]\tBatch Time 0.847 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0191 (1.2647)\tTop-5 Accuracy 99.493 (97.956)\n",
      "Epoch: [0][5500/6958]\tBatch Time 0.821 (0.842)\tData Load Time 0.001 (0.000)\tLoss 1.0627 (1.2608)\tTop-5 Accuracy 99.395 (97.980)\n",
      "Epoch: [0][5600/6958]\tBatch Time 0.839 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0770 (1.2571)\tTop-5 Accuracy 99.111 (98.004)\n",
      "Epoch: [0][5700/6958]\tBatch Time 0.841 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0510 (1.2534)\tTop-5 Accuracy 99.360 (98.027)\n",
      "Epoch: [0][5800/6958]\tBatch Time 0.842 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0071 (1.2499)\tTop-5 Accuracy 99.267 (98.050)\n",
      "Epoch: [0][5900/6958]\tBatch Time 0.843 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0220 (1.2464)\tTop-5 Accuracy 99.465 (98.072)\n",
      "Epoch: [0][6000/6958]\tBatch Time 0.839 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0371 (1.2430)\tTop-5 Accuracy 99.212 (98.093)\n",
      "Epoch: [0][6100/6958]\tBatch Time 0.849 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0234 (1.2397)\tTop-5 Accuracy 99.554 (98.113)\n",
      "Epoch: [0][6200/6958]\tBatch Time 0.842 (0.842)\tData Load Time 0.001 (0.000)\tLoss 1.0314 (1.2365)\tTop-5 Accuracy 99.423 (98.134)\n",
      "Epoch: [0][6300/6958]\tBatch Time 0.838 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0260 (1.2333)\tTop-5 Accuracy 99.547 (98.153)\n",
      "Epoch: [0][6400/6958]\tBatch Time 0.855 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0416 (1.2302)\tTop-5 Accuracy 99.156 (98.172)\n",
      "Epoch: [0][6500/6958]\tBatch Time 0.823 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0570 (1.2271)\tTop-5 Accuracy 99.379 (98.191)\n",
      "Epoch: [0][6600/6958]\tBatch Time 0.846 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0500 (1.2241)\tTop-5 Accuracy 99.346 (98.209)\n",
      "Epoch: [0][6700/6958]\tBatch Time 0.842 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0336 (1.2211)\tTop-5 Accuracy 99.363 (98.226)\n",
      "Epoch: [0][6800/6958]\tBatch Time 0.857 (0.842)\tData Load Time 0.000 (0.000)\tLoss 0.9871 (1.2182)\tTop-5 Accuracy 99.482 (98.243)\n",
      "Epoch: [0][6900/6958]\tBatch Time 0.822 (0.842)\tData Load Time 0.000 (0.000)\tLoss 1.0485 (1.2154)\tTop-5 Accuracy 99.357 (98.260)\n",
      "Validation: [0/142]\tBatch Time 0.783 (0.783)\tLoss 0.9682 (0.9682)\tTop-5 Accuracy 99.593 (99.593)\t\n",
      "Validation: [100/142]\tBatch Time 0.528 (0.533)\tLoss 0.9907 (0.9860)\tTop-5 Accuracy 99.670 (99.550)\t\n",
      "\n",
      " * LOSS - 0.986, TOP-5 ACCURACY - 99.543\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-a32bac5df49a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mbest_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecent_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mbest_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{save_dir}/{model_name}.pt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# util.py로 옮겨야하나?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32melse\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mepochs_since_improvement\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "best_loss = 1.0\n",
    "best_epoch = -1\n",
    "save_dir = './save_model'\n",
    "for epoch in range(start_epoch, epochs): # epochs \n",
    "    # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "    if epochs_since_improvement == 20:\n",
    "        break\n",
    "    if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "        adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "        if fine_tune_encoder:\n",
    "            adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "    # One epoch's training\n",
    "    train(train_loader=train_loader,\n",
    "          encoder=encoder,\n",
    "          decoder=decoder,\n",
    "          criterion=criterion,\n",
    "          encoder_optimizer=encoder_optimizer,\n",
    "          decoder_optimizer=decoder_optimizer,\n",
    "          epoch=epoch)\n",
    "    \n",
    "    \n",
    "        \n",
    "    recent_loss = validate(val_loader=val_loader,\n",
    "                            encoder=encoder,\n",
    "                            decoder=decoder,\n",
    "                            criterion=criterion)\n",
    "    '''\n",
    "    # Check if there was an improvement\n",
    "    is_best = recent_loss.val < best_loss\n",
    "    best_loss = max(recent_loss.val, best_loss) # max?\n",
    "    if not is_best:\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "    else:\n",
    "        epochs_since_improvement = 0\n",
    "\n",
    "    # Save checkpoint\n",
    "    save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "                    decoder_optimizer, recent_loss, is_best)\n",
    "    '''\n",
    "    if recent_loss.val < best_loss :\n",
    "        epochs_since_improvement = 0\n",
    "        best_loss = recent_loss.val\n",
    "        best_epoch = epoch\n",
    "       # torch.save(model, f'{save_dir}/{model_name}.pt') # util.py로 옮겨야하나?\n",
    "    else :\n",
    "        epochs_since_improvement += 1\n",
    "        print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_seq(image, beam_size=5):    \n",
    "    k = beam_size\n",
    "\n",
    "    # Move to GPU device, if available\n",
    "    image = image.to(device)  # (1, 3, 256, 256)\n",
    "\n",
    "    # Encode\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "    \n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just <start>\n",
    "    k_prev_words = torch.LongTensor([[0]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just <start>\n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Lists to store completed sequences and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
    "    while True:\n",
    "\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "        awe = gate * awe\n",
    "\n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words // vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        # Add new words to sequences\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach <end>)?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != 1]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 80:\n",
    "            break\n",
    "        step += 1\n",
    "        \n",
    "    if len(complete_seqs_scores) == 0: # 훈련이 덜된 경우, 발생 가능\n",
    "        print('no sequence left')\n",
    "        return ''\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    #predicted_seq = ''.join([dataset.get_vocab()[1][num] for num in seq ])[1:-1]\n",
    "    \n",
    "    return np.array(complete_seqs)[np.argsort(complete_seqs_scores)[::-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = torch.utils.data.DataLoader(val_set, batch_size=1, shuffle=True, num_workers=workers, pin_memory=True)\n",
    "\n",
    "predicted_seqs = []\n",
    "true_seqs = []\n",
    "\n",
    "beam_size = 20\n",
    "\n",
    "for imgs, caps, caplens in tqdm(val_loader):\n",
    "    \n",
    "    predicted_smiles = predict_seq(imgs, beam_size)\n",
    "    \n",
    "    for i in range(len(predicted_smiles)): \n",
    "        predicted_smile = ''.join([dataset.get_vocab()[1][num] for num in predicted_smiles[i] ])[1:-1]\n",
    "        m = Chem.MolFromSmiles(predicted_smile)\n",
    "        \n",
    "        if m != None:\n",
    "            break\n",
    "    \n",
    "    predicted_seqs.append(predicted_seqs)\n",
    "    true_seqs.append(''.join([dataset.get_vocab()[1][num.item()] for num in caps.squeeze() if num != 2 ])[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-e664353dcb32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrue_seqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcount\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'val_accuracy : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted_seqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for rid, pred in enumerate(predicted_seqs):    \n",
    "    if true_seqs[rid] == pred:\n",
    "        count+=1\n",
    "print('val_accuracy : ', count/len(predicted_seqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증 데이터셋 Tanimoto Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = []\n",
    "for i, pred in enumerate(predicted_seqs):\n",
    "    m1 = Chem.MolFromSmiles(true_seqs[i])\n",
    "    m2 = Chem.MolFromSmiles(pred)\n",
    "    \n",
    "    if m2 != None:\n",
    "        fp1 = Chem.RDKFingerprint(m1)\n",
    "        fp2 = Chem.RDKFingerprint(m2)\n",
    "\n",
    "        similarity = DataStructs.FingerprintSimilarity(fp1,fp2)\n",
    "    else:\n",
    "        similarity = 0\n",
    "    score.append(similarity)\n",
    "    \n",
    "print('val_similarity :', np.mean(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_idx = []\n",
    "for i, pred in enumerate(preds):\n",
    "    m = Chem.MolFromSmiles(pred)\n",
    "    if m == None:\n",
    "        error_idx.append(i)\n",
    "error_idx = np.array(error_idx)\n",
    "error_idx_ = error_idx.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_error = []\n",
    "while True:\n",
    "    error_idx_dict = {}\n",
    "    for i, e in enumerate(error_idx_):\n",
    "        error_idx_dict[i] = e\n",
    "        \n",
    "    img_name_test_ = np.array(test_img_path)[error_idx_]\n",
    "    dataset_test_ = tf.data.Dataset.from_tensor_slices((img_name_test_))\n",
    "    dataset_test_ = dataset_test_.map(lambda item1: tf.numpy_function(map_func_pred, [item1], [tf.float32]), num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset_test_ = dataset_test_.batch(BATCH_SIZE)\n",
    "    dataset_test_ = dataset_test_.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    \n",
    "    test_result_ = []\n",
    "    for batch in dataset_test_:\n",
    "        test_result_.extend(predict_(batch[0]).T)\n",
    "    test_result_ = np.array(test_result_)\n",
    "\n",
    "    preds_ = []\n",
    "    for rid in range(test_result_.shape[0]):\n",
    "        pred = ''.join([tokenizer.index_word[i] for i in test_result_[rid] if i not in [0]])\n",
    "        pred = pred.split('>')[0]\n",
    "        preds_.append(pred)\n",
    "    \n",
    "    for i, pred in enumerate(preds_):\n",
    "        m = Chem.MolFromSmiles(pred)\n",
    "        if m != None:\n",
    "            preds[error_idx_dict[i]] = pred\n",
    "            drop_idx = np.where(error_idx==error_idx_dict[i])[0]\n",
    "            drop_error.append(drop_idx[0])\n",
    "    error_idx_ = np.delete(error_idx, drop_error)\n",
    "    clear_output(wait=True)\n",
    "    print(len(list(drop_error)), '/', error_idx.shape[0])\n",
    "    \n",
    "    if error_idx.shape[0]-len(list(drop_error)) < 10 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission['SMILES'] = np.array(preds)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('Dacon_baseline.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
